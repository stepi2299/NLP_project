{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Bert Training\n",
    "\n"
   ],
   "metadata": {
    "id": "w9tDiol3tq7i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters"
   ],
   "metadata": {
    "id": "jlhOblrEuarf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DATASET_PATH = \"/content/meld.csv\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# data\n",
    "SAMPLE = None\n",
    "X_LABEL = 'Utterance'  # Utterance, Transcription\n",
    "Y_LABEL = 'Sentiment'\n",
    "Y_CLASSES = ['negative', 'positive', \"neutral\"]\n",
    "TRANSCRIPT_MATCH_THRESHOLD = 0.2\n",
    "\n",
    "# model\n",
    "MODEL_NAME = 'tiny_bert'  # 'bert', 'distil_bert', 'tiny_bert'\n",
    "DROPOUT_PROB = 0.8\n",
    "\n",
    "# training\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE=1e-5\n",
    "MAX_LENGTH = 100\n",
    "WEIGHT_DECAY= 2e-4  # 2e-5"
   ],
   "metadata": {
    "id": "cWqBM4wXuZxA",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dependencies"
   ],
   "metadata": {
    "id": "3qBbRlNbuLXA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "id": "mhoq_dRMwIis"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir \"models\""
   ],
   "metadata": {
    "id": "qfm0N8wD8rJY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup, AutoModel, AutoTokenizer, DistilBertTokenizer, DistilBertModel, PreTrainedTokenizerBase\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from difflib import SequenceMatcher\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple\n",
    "from abc import abstractmethod"
   ],
   "metadata": {
    "id": "DeJ2hTDluFiC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "id": "jyANSM1VtwN5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIykqpiUtcJW"
   },
   "outputs": [],
   "source": [
    "class MeldDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, x_label: str, y_label: str, max_length: int, augment=None):\n",
    "        self.x_list: np.ndarray = df[x_label].to_numpy()\n",
    "        ohe = OneHotEncoder()\n",
    "        codes = df[y_label].to_numpy()\n",
    "        codes = np.expand_dims(codes, axis=1)\n",
    "        self.y_list: np.ndarray = ohe.fit_transform(codes).toarray()\n",
    "        self.categories = ohe.categories_\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.x_list[item]\n",
    "        if self.augment:\n",
    "            text = self.augment(text)\n",
    "        encoded_dict: dict = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        inputs_ids = encoded_dict['input_ids'].reshape(-1)\n",
    "        attention_mask = encoded_dict['attention_mask'].reshape(-1)\n",
    "        y_tensor = torch.tensor(self.y_list[item])\n",
    "        return inputs_ids, attention_mask, y_tensor, text"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_data_loader(dataset: Dataset, batch_size: int):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )"
   ],
   "metadata": {
    "id": "pQbzHflOtn3G"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_dataset_based_on_class(df: pd.DataFrame, y_label: str, y_classes: list) -> pd.DataFrame:\n",
    "    all_possibles_classes = df[y_label].unique()\n",
    "    if len(all_possibles_classes) == len(y_classes):\n",
    "        return df\n",
    "    elif len(y_classes) < 2:\n",
    "        print(\"Minimal number of analyzed class is 2, setting analyzed class into two -> positive and negative\")\n",
    "        y_classes = ['negative', 'positive']\n",
    "    class_to_delete = set(all_possibles_classes) - set(y_classes)\n",
    "    df = df.loc[df[y_label] != list(class_to_delete)[0]].reset_index(drop=True)\n",
    "    return df"
   ],
   "metadata": {
    "id": "cCASfwP9HCjt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_junk_transcriptions(df_row):\n",
    "    s = SequenceMatcher(None, df_row['Utterance'], df_row['Transcription'])\n",
    "    return s.ratio()"
   ],
   "metadata": {
    "id": "mK1U8yK1HCju"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_data(df: pd.DataFrame, y_label: str, y_classes: list, match_threshold: float, sample: int = None):\n",
    "    df = df.dropna()\n",
    "    df = df[df.apply(lambda row: remove_junk_transcriptions(row),\n",
    "                     axis=1) > match_threshold]\n",
    "    df = prepare_dataset_based_on_class(df, y_label=y_label,\n",
    "                                        y_classes=y_classes)\n",
    "    # limit dataframe length\n",
    "    if sample:\n",
    "        df = df.head(sample)\n",
    "    return df"
   ],
   "metadata": {
    "id": "HHNmbtu8HCju"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bert Model"
   ],
   "metadata": {
    "id": "4uoeEUH4t7vQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, model_name: str, dropout_prob: float):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.module_name = model_name\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomBertClassifier(BertClassifier):\n",
    "    # bert core + dropout + one layer feed-forward\n",
    "    def __init__(self, model_name, dropout_prob, n_classes):\n",
    "        super(CustomBertClassifier, self).__init__(model_name=model_name, dropout_prob=dropout_prob)\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)"
   ],
   "metadata": {
    "id": "L-0Xtx1st35o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomDistilBertClassifier(BertClassifier):\n",
    "    def __init__(self, model_name, dropout_prob, n_classes):\n",
    "        super(CustomDistilBertClassifier, self).__init__(model_name=model_name, dropout_prob=dropout_prob)\n",
    "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )[0]\n",
    "        pooled_output = pooled_output[:, 0]\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomTinyBertClassifier(BertClassifier):\n",
    "    def __init__(self, model_name, dropout_prob, n_classes):\n",
    "        super(CustomTinyBertClassifier, self).__init__(model_name=model_name, dropout_prob=dropout_prob)\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )[0]\n",
    "        pooled_output = pooled_output[:, 0]\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BertFactory:\n",
    "    POSSIBLE_MODEL_NAMES = ['bert', 'distil_bert', 'tiny_bert']\n",
    "\n",
    "    @staticmethod\n",
    "    def create(model_name: str, dropout_prob: float, n_classes: int) -> Tuple[PreTrainedTokenizerBase, BertClassifier]:\n",
    "        if not model_name.lower() in BertFactory.POSSIBLE_MODEL_NAMES:\n",
    "            raise Exception(f\"Received model is not supported, received: {model_name},\"\n",
    "                            f\" supported: {BertFactory.POSSIBLE_MODEL_NAMES}\")\n",
    "        if model_name == \"bert\":\n",
    "            whole_model_name = \"bert-base-uncased\"\n",
    "            return (BertTokenizer.from_pretrained(whole_model_name),\n",
    "                    CustomBertClassifier(whole_model_name, dropout_prob, n_classes))\n",
    "        elif model_name == \"distil_bert\":\n",
    "            whole_model_name = \"distilbert-base-uncased\"\n",
    "            return (DistilBertTokenizer.from_pretrained(whole_model_name),\n",
    "                    CustomDistilBertClassifier(whole_model_name, dropout_prob, n_classes))\n",
    "        else:\n",
    "            whole_model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "            return (AutoTokenizer.from_pretrained(whole_model_name),\n",
    "                    CustomTinyBertClassifier(whole_model_name, dropout_prob, n_classes))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building model"
   ],
   "metadata": {
    "id": "42oxOTGWvdlF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class BertClassifierInterface:\n",
    "    def __init__(self, model_name, dropout_prob, n_classes, lr=1e-5, wg=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n_classes = n_classes\n",
    "        self.tokenizer, self.model = BertFactory.create(model_name, dropout_prob, n_classes)\n",
    "        self.model.to(self.device)\n",
    "        self.loss_function = nn.CrossEntropyLoss().to(self.device)\n",
    "        if model_name == \"distil_bert\":\n",
    "            self.loss_function = nn.BCEWithLogitsLoss().to(self.device)\n",
    "        if wg:\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=lr, weight_decay=wg)\n",
    "        else:\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        self.mapper = []\n",
    "        self.history = []\n",
    "\n",
    "    @staticmethod\n",
    "    def data_preprocess(df: pd.DataFrame, tokenizer, x_label: str, y_label: str,\n",
    "                 max_length: int, batch_size=16, transform=None):\n",
    "        df_train, df_val = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "        train_dataset = MeldDataset(df_train, tokenizer, x_label, y_label, max_length, transform)\n",
    "        val_dataset = MeldDataset(df_val, tokenizer, x_label, y_label, max_length, transform)\n",
    "        assert np.all(np.equal(train_dataset.categories[0], val_dataset.categories[0]))\n",
    "        mapper = train_dataset.categories[0]\n",
    "        train_data_loader: DataLoader = create_data_loader(train_dataset, batch_size)\n",
    "        val_data_loader: DataLoader = create_data_loader(val_dataset, batch_size)\n",
    "        return train_data_loader, val_data_loader, mapper\n",
    "\n",
    "    def train(self, df: pd.DataFrame, x_label: str, y_label: str,\n",
    "                 max_length: int = 70, batch_size: int = 16, epochs: int = 10, transform=None):\n",
    "        train_data_loader, val_data_loader, self.mapper = self.data_preprocess(df, self.tokenizer, x_label, y_label, max_length, batch_size, transform)\n",
    "        total_steps: int = len(train_data_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=self.optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        best_acc: float = 0\n",
    "        for epoch_i in range(epochs):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\n",
    "            print('Training...')\n",
    "\n",
    "            train_acc, train_loss = self._train(\n",
    "                data_loader=train_data_loader,\n",
    "                scheduler=scheduler\n",
    "            )\n",
    "\n",
    "            print(\"  Train accuracy: {0:.2f}\".format(train_acc))\n",
    "            print(\"  Train loss: {0:.2f}\".format(train_loss))\n",
    "\n",
    "            print('Running validation...')\n",
    "\n",
    "            val_acc, val_loss = self.evaluate(\n",
    "                data_loader=val_data_loader,\n",
    "            )\n",
    "\n",
    "            print(\"  Validation accuracy: {0:.2f}\".format(val_acc))\n",
    "            print(\"  Validation loss: {0:.2f}\".format(val_loss))\n",
    "            current_history = [train_acc, train_loss, val_acc, val_loss]\n",
    "            self.history.append(current_history)\n",
    "\n",
    "            # save model state with best accuracy\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(self.model.state_dict(), '/content/models/best_model.bin')\n",
    "\n",
    "\n",
    "    def _train(self, data_loader: DataLoader, scheduler):\n",
    "\n",
    "        model = self.model.train()\n",
    "\n",
    "        losses = []\n",
    "        correct_predictions: int = 0\n",
    "\n",
    "        loop = tqdm(data_loader)\n",
    "        for idx, d in enumerate(loop):\n",
    "            input_ids = d[0].to(self.device)\n",
    "            attention_mask = d[1].to(self.device)\n",
    "            targets = d[2].to(self.device)\n",
    "\n",
    "            # get model outputs\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            _, correct = torch.max(targets, dim=1)\n",
    "            correct_predictions += sum(torch.eq(predictions, correct))\n",
    "\n",
    "            loss = self.loss_function(outputs, targets)\n",
    "                \n",
    "\n",
    "            losses.append(loss.item())\n",
    "            # Backward prop\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Descent\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        return float(correct_predictions) / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "\n",
    "    def evaluate(self, data_loader: DataLoader):\n",
    "        # set mode\n",
    "        model = self.model.eval()\n",
    "\n",
    "        losses = []\n",
    "        correct_predictions: int = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loop = tqdm(data_loader)\n",
    "            for idx, d in enumerate(loop):\n",
    "                input_ids = d[0].to(self.device)\n",
    "                attention_mask = d[1].to(self.device)\n",
    "                targets = d[2].to(self.device)\n",
    "\n",
    "                # get model outputs\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                _, correct_preds = torch.max(targets, dim=1)\n",
    "                correct_predictions += sum(torch.eq(preds, correct_preds))\n",
    "                loss = self.loss_function(outputs, targets)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        return float(correct_predictions) / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "    def predict(self, data_loader: DataLoader):\n",
    "        # set mode\n",
    "        model = self.model.eval()\n",
    "\n",
    "        x_values = []\n",
    "        y_predictions = []\n",
    "        y_probabilities = []\n",
    "        y_actual = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loop = tqdm(data_loader)\n",
    "            for idx, d in enumerate(loop):\n",
    "                input_ids = d[0].to(self.device)\n",
    "                attention_mask = d[1].to(self.device)\n",
    "                targets = d[2].to(self.device)\n",
    "                x_vals = d[3]\n",
    "\n",
    "                # get model outputs\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "                x_values.extend(x_vals)\n",
    "                y_predictions.extend(preds)\n",
    "                y_probabilities.extend(outputs)\n",
    "                y_actual.extend(targets)\n",
    "\n",
    "        y_predictions = torch.stack(y_predictions).cpu()\n",
    "        y_probabilities = torch.stack(y_probabilities).cpu()\n",
    "        y_actual = torch.stack(y_actual).cpu()\n",
    "\n",
    "        return x_values, y_predictions, y_probabilities, y_actual\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model.load_state_dict(torch.load(f'/content/models/{filename}.bin'))\n",
    "\n",
    "    def save(self, filename=\"final_model\"):\n",
    "        torch.save(self.model.state_dict(), f'/content/models/{filename}.bin')\n"
   ],
   "metadata": {
    "id": "2QF9cgiZvc4X"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "id": "RX-uFJuiv-1K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df: pd.DataFrame = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "df = process_data(df, Y_LABEL, Y_CLASSES, TRANSCRIPT_MATCH_THRESHOLD, SAMPLE)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)"
   ],
   "metadata": {
    "id": "V5K9Umahv4sC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# limit dataframe length\n",
    "if SAMPLE:\n",
    "    df = df.head(SAMPLE)"
   ],
   "metadata": {
    "id": "rDPVuqCkwWuO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)"
   ],
   "metadata": {
    "id": "ejpblLPLwXH2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = BertClassifierInterface(MODEL_NAME, DROPOUT_PROB, len(Y_CLASSES), lr=LEARNING_RATE, wg=WEIGHT_DECAY)"
   ],
   "metadata": {
    "id": "Cs865mGiHCjx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.train(df_train, X_LABEL, Y_LABEL, MAX_LENGTH, BATCH_SIZE, EPOCHS)"
   ],
   "metadata": {
    "id": "FqJiBaKttsVR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "hist_time = model.history\n",
    "epochs = list(range(1, len(hist_time)+1))\n",
    "\n",
    "train_acc = [nested[0] for nested in hist_time]\n",
    "train_loss = [nested[1] for nested in hist_time]\n",
    "val_acc = [nested[2] for nested in hist_time]\n",
    "val_loss = [nested[3] for nested in hist_time]\n",
    "\n",
    "\n",
    "# Plotting these values\n",
    "plt.plot(epochs, train_acc, label='Training Accuracy')\n",
    "plt.plot(epochs, train_loss, label='Training Loss')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "\n",
    "# Adding a title\n",
    "plt.title('DistilBert Model Training')\n",
    "\n",
    "# Adding x and y label\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss and Accuracy')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "ylVnto7ZSPou"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false,
    "id": "hLqNrkR-HCjx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dataset = MeldDataset(df_test, model.tokenizer, X_LABEL, Y_LABEL, MAX_LENGTH)\n",
    "assert np.all(np.equal(test_dataset.categories[0], model.mapper))\n",
    "mapper = test_dataset.categories\n",
    "test_data_loader = create_data_loader(test_dataset, BATCH_SIZE)"
   ],
   "metadata": {
    "id": "dQYgybdgHCjx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_acc, _ = model.evaluate(test_data_loader)\n",
    "\n",
    "print(\"  Test accuracy: {0:.2f}\".format(test_acc))"
   ],
   "metadata": {
    "id": "qeOGVB9JHCjx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_val, y_pred, y_probs, y_test = model.predict(test_data_loader)\n",
    "\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "print(\"0:\", test_dataset.categories[0][0], \", 1:\", test_dataset.categories[0][1], \", 2:\", test_dataset.categories[0][2])"
   ],
   "metadata": {
    "id": "FgL1-sJGHCjx"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "FCHCyaXOK5kh"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
