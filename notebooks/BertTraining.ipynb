{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMqiIza4nBkD+ClzYRkzu3L"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Bert Training\n",
    "\n"
   ],
   "metadata": {
    "id": "w9tDiol3tq7i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters"
   ],
   "metadata": {
    "id": "jlhOblrEuarf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DATASET_PATH = \"/content/meld.csv\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# data\n",
    "SAMPLE = 1000\n",
    "X_LABEL = 'Utterance'\n",
    "Y_LABEL = 'Sentiment'\n",
    "Y_CLASSES = ['negative', 'positive', \"neutral\"]\n",
    "\n",
    "# model\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "DROPOUT_PROB = 0.3\n",
    "\n",
    "# training\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 44"
   ],
   "metadata": {
    "id": "cWqBM4wXuZxA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dependencies"
   ],
   "metadata": {
    "id": "3qBbRlNbuLXA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "id": "mhoq_dRMwIis"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ],
   "metadata": {
    "id": "DeJ2hTDluFiC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "id": "jyANSM1VtwN5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIykqpiUtcJW"
   },
   "outputs": [],
   "source": [
    "class MeldDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, x_label: str, y_label: str, max_length: int, augment=None):\n",
    "        self.x_list: np.ndarray = df[x_label].to_numpy()\n",
    "        ohe = OneHotEncoder()\n",
    "        codes = df[y_label].to_numpy()\n",
    "        codes = np.expand_dims(codes, axis=1)\n",
    "        self.y_list: np.ndarray = ohe.fit_transform(codes).toarray()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.x_list[item]\n",
    "        if self.augment:\n",
    "            text = self.augment(text)\n",
    "        encoded_dict: dict = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        inputs_ids = encoded_dict['input_ids'].reshape(-1)\n",
    "        attention_mask = encoded_dict['attention_mask'].reshape(-1)\n",
    "        y_tensor = torch.tensor(self.y_list[item])\n",
    "        return inputs_ids, attention_mask, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_data_loader(dataset: Dataset, batch_size: int):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )"
   ],
   "metadata": {
    "id": "pQbzHflOtn3G"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preparing_dataset_based_on_class(df: pd.DataFrame, y_label: str, y_classes: list) -> pd.DataFrame:\n",
    "    all_possibles_classes = df[y_label].unique()\n",
    "    if len(all_possibles_classes) == len(y_classes):\n",
    "        return df\n",
    "    elif len(y_classes) < 2:\n",
    "        print(\"Minimal number of analyzed class is 2, setting analyzed class into two -> positive and negative\")\n",
    "        y_classes = ['negative', 'positive']\n",
    "    class_to_delete = set(all_possibles_classes) - set(y_classes)\n",
    "    df = df.loc[df[y_label] != list(class_to_delete)[0]].reset_index(drop=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bert Model"
   ],
   "metadata": {
    "id": "4uoeEUH4t7vQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomBertClassifier(nn.Module):\n",
    "    # bert core + dropout + one layer feed-forward\n",
    "    def __init__(self, model_name, dropout_prob, n_classes=2):\n",
    "        super(CustomBertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)"
   ],
   "metadata": {
    "id": "L-0Xtx1st35o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building model"
   ],
   "metadata": {
    "id": "42oxOTGWvdlF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model: nn.Module, data_loader: DataLoader, loss_fn, optim, dev: torch.device, sched, n_samples: int):\n",
    "    # set mode\n",
    "    model = model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    loop = tqdm(data_loader)\n",
    "    for idx, d in enumerate(loop):\n",
    "        input_ids = d[0].to(dev)\n",
    "        attention_mask = d[1].to(dev)\n",
    "        targets = d[2].to(dev)\n",
    "\n",
    "        # get model outputs\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "        _, correct = torch.max(targets, dim=1)\n",
    "        correct_predictions += sum(torch.eq(predictions, correct))\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backward prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Descent\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "        sched.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    return float(correct_predictions) / n_samples, np.mean(losses)\n"
   ],
   "metadata": {
    "id": "2QF9cgiZvc4X"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model: CustomBertClassifier, data_loader: DataLoader, loss_fn, dev: torch.device, n_samples: int):\n",
    "    # set mode\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(data_loader)\n",
    "        for idx, d in enumerate(loop):\n",
    "            input_ids = d[0].to(dev)\n",
    "            attention_mask = d[1].to(dev)\n",
    "            targets = d[2].to(dev)\n",
    "\n",
    "            # get model outputs\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            _, correct = torch.max(targets, dim=1)\n",
    "            correct_predictions += sum(torch.eq(predictions, correct))\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return float(correct_predictions) / n_samples, np.mean(losses)"
   ],
   "metadata": {
    "id": "fGi4tVOyvsgy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "id": "RX-uFJuiv-1K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df: pd.DataFrame = pd.read_csv(DATASET_PATH)\n",
    "df: pd.DataFrame = preparing_dataset_based_on_class(df, y_label=Y_LABEL, y_classes=Y_CLASSES)"
   ],
   "metadata": {
    "id": "V5K9Umahv4sC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# limit dataframe length\n",
    "if SAMPLE:\n",
    "    df = df.head(SAMPLE)"
   ],
   "metadata": {
    "id": "rDPVuqCkwWuO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n"
   ],
   "metadata": {
    "id": "ejpblLPLwXH2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "ajF3V9R2xAYE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ],
   "metadata": {
    "id": "TCd8wOo3xG02"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset: Dataset = MeldDataset(df_train, tokenizer, X_LABEL, Y_LABEL, MAX_LENGTH)\n",
    "val_dataset: Dataset = MeldDataset(df_val, tokenizer, X_LABEL, Y_LABEL, MAX_LENGTH)\n",
    "test_dataset: Dataset = MeldDataset(df_test, tokenizer, X_LABEL, Y_LABEL, MAX_LENGTH)"
   ],
   "metadata": {
    "id": "SEXe_987xINi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_data_loader: DataLoader = create_data_loader(train_dataset, BATCH_SIZE)\n",
    "val_data_loader: DataLoader = create_data_loader(val_dataset, BATCH_SIZE)\n",
    "test_data_loader: DataLoader = create_data_loader(test_dataset, BATCH_SIZE)\n"
   ],
   "metadata": {
    "id": "AAnA1bV_xL8h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "bert_model = BertModel.from_pretrained(MODEL_NAME)\n",
    "custom_model = CustomBertClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    dropout_prob=DROPOUT_PROB,\n",
    "    n_classes=len(Y_CLASSES)\n",
    ").to(device)\n"
   ],
   "metadata": {
    "id": "K4_xjaiUxOnx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "params: list[tuple] = list(custom_model.named_parameters())\n",
    "optimizer = AdamW(custom_model.parameters(), lr=2e-5)"
   ],
   "metadata": {
    "id": "X7PL39GkxTEh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "total_steps: int = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ],
   "metadata": {
    "id": "vGK5rP9uxX6i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "best_acc: float = 0"
   ],
   "metadata": {
    "id": "BNHAa9WhxaO0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch_i in range(EPOCHS):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "\n",
    "    print('Training...')\n",
    "\n",
    "    train_acc, train_loss = train(\n",
    "        model=custom_model,\n",
    "        data_loader=train_data_loader,\n",
    "        loss_fn=loss_function,\n",
    "        optim=optimizer,\n",
    "        dev=device,\n",
    "        sched=scheduler,\n",
    "        n_samples=len(df_train)\n",
    "    )\n",
    "\n",
    "    print(\"  Train accuracy: {0:.2f}\".format(train_acc))\n",
    "    print(\"  Train loss: {0:.2f}\".format(train_loss))\n",
    "\n",
    "    print('Running validation...')\n",
    "\n",
    "    val_acc, val_loss = evaluate(\n",
    "        model=custom_model,\n",
    "        data_loader=val_data_loader,\n",
    "        loss_fn=loss_function,\n",
    "        dev=device,\n",
    "        n_samples=len(df_val)\n",
    "    )\n",
    "\n",
    "    print(\"  Validation accuracy: {0:.2f}\".format(val_acc))\n",
    "    print(\"  Validation loss: {0:.2f}\".format(val_loss))\n",
    "\n",
    "    # save model state with best accuracy\n",
    "    if val_acc > best_acc:\n",
    "        torch.save(custom_model.state_dict(), 'best_model.bin')\n",
    "        best_acc = val_acc"
   ],
   "metadata": {
    "id": "tMzbjUDPxddO"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
